{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "southern-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import shutil\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "import torch\n",
    "from flask import Flask, request\n",
    "from flask import render_template\n",
    "from flask_ngrok import run_with_ngrok\n",
    "ws_driver = CkipWordSegmenter(level=3,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adopted-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fin_word.json', 'r', encoding='utf-8') as f:\n",
    "    word2idx = json.load(f)\n",
    "with open('fin_tag.json', 'r', encoding='utf-8') as f:\n",
    "    tag2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "thick-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['<pad>'] = 0\n",
    "idx2word = {word2idx[w]:w for w in word2idx}\n",
    "idx2tag = {tag2idx[t]:t for t in tag2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "entitled-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\u3000\", \"\", string)\n",
    "    string = re.sub(r\"\\s{1,}\", \"\", string)\n",
    "    string = re.sub(r\"\\r\", \"\", string)\n",
    "    string = re.sub(r\"\\n\", \"\", string)\n",
    "    string = re.sub(r\"【[^【】]+】\",\"\",string)\n",
    "    string = re.sub(r\"（[^（）]+）\",\"\",string)\n",
    "    string = re.sub(r\"「[^「」]+」\",\"\",string)\n",
    "    string = re.sub(r\"\\([^\\(\\)]+\\)\",\"\",string)\n",
    "    if '審判長' in string:\n",
    "        string = string.split('審判長')[0]\n",
    "    else:\n",
    "        string = string.split('書記官')[0]\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "greenhouse-alcohol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j_fs = []\n",
    "\n",
    "for file in os.listdir('金融判決'):\n",
    "    with open(f'金融判決/{file}' , 'r' , encoding = 'utf-8') as f:\n",
    "        j_f = json.load(f)\n",
    "        try:\n",
    "            if len(clean_str(j_f['judgement'])) <3000 and len(j_f['relatedIssues']) < 15 and '由於裁判書全文大於' not in j_f['judgement']:\n",
    "                j_fs.append(file)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "industrial-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('驗證的判決'):\n",
    "    with open(f'驗證的判決/{file}' , 'r' , encoding = 'utf-8') as f:\n",
    "        j_f = json.load(f)\n",
    "        with open(f'驗證的判決/{file}.txt','w',encoding = 'utf-8') as fw:\n",
    "            fw.write(j_f['judgement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "passive-garbage",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-56d5dd17d4f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj_fs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'judgement'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprocess_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "text = j_fs[idx]['judgement']\n",
    "process_text = clean_str(text)\n",
    "len(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-differential",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "for law in j_fs[idx]['relatedIssues']:\n",
    "    str_ = law['lawName']+law['issueRef']\n",
    "    str_ = str_.strip()\n",
    "    if str_ in tags:\n",
    "        continue\n",
    "    if str_ in tag2idx:\n",
    "        tags.append(str_)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_idx = [tag2idx[t] for t in tags]\n",
    "tags_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "blind-prior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inp_len(x):\n",
    "    if len(x) > 0:\n",
    "        return x\n",
    "\n",
    "doc = []\n",
    "for t in process_text.split('，'):\n",
    "    if '。' in t:\n",
    "        doc.extend(t.split('。'))\n",
    "    else:\n",
    "        doc.append(t)\n",
    "doc = list(filter(inp_len,doc))\n",
    "docs = [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sound-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|█████████████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 27443.10it/s]\n",
      "Inference: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.33it/s]\n"
     ]
    }
   ],
   "source": [
    "docs_p = []\n",
    "sentence_length = []\n",
    "doc_length = []\n",
    "# ws = ws_driver(docs , batch_size=16)\n",
    "for doc in docs:\n",
    "    ws = ws_driver(doc)\n",
    "    d_p = []\n",
    "    s_len = []\n",
    "    for voc in ws:\n",
    "        d_p.append(voc)\n",
    "        s_len.append(len(voc))\n",
    "    sentence_length.append(s_len)\n",
    "    docs_p.append(d_p)\n",
    "    doc_length.append(len(s_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exterior-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_w2i = [[list(map( lambda s:word2idx.get(s,word2idx['<unk>']) ,sent )) for sent in doc_p] for doc_p in docs_p ]\n",
    "\n",
    "dict_han = {\n",
    "    'docs':docs_w2i,\n",
    "    'sentences_per_document':doc_length,\n",
    "    'words_per_sentence':sentence_length,\n",
    "    'labels':[tags_idx],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compliant-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader ,SubsetRandomSampler ,ConcatDataset ,Dataset\n",
    "\n",
    "word_limit = 40 ## 每句話的最長長度\n",
    "sentence_limit = 80 ## 每篇判決的句子數目\n",
    "\n",
    "class HANDataset(Dataset):\n",
    "    def __init__(self,data,word_pad_idx,num_classes , word_limit = 40 ,sentence_limit = 80 ):\n",
    "        # Load data\n",
    "        self.data = data\n",
    "        self.word_limit = word_limit\n",
    "        self.sentence_limit = sentence_limit\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __getitem__(self, id_doc):\n",
    "        return (self.data['docs'][id_doc], \\\n",
    "               self.data['sentences_per_document'][id_doc], \\\n",
    "               self.data['words_per_sentence'][id_doc], \\\n",
    "               self.data['labels'][id_doc])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['labels'])\n",
    "    \n",
    "    def one_hot(self,tag):\n",
    "        bs_oh_label = torch.tensor([0.0] * self.num_classes)\n",
    "        for t in tag:\n",
    "            bs_oh_label += torch.eye(self.num_classes)[t]\n",
    "        return bs_oh_label.tolist()\n",
    "    \n",
    "    def turncut_dlen(self,x):\n",
    "        if len(x) > self.max_doc_len:\n",
    "            return x[:self.max_doc_len]\n",
    "        elif len(x) < self.max_doc_len:\n",
    "            return x + [0] * (self.max_doc_len-len(x))\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def max_slen(self,x):\n",
    "        if x > self.max_sent_len:\n",
    "            return self.max_sent_len\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def max_dlen(self,x):\n",
    "        if x > self.max_doc_len:\n",
    "            return self.max_doc_len\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def collate_fn(self, col_datasets):\n",
    "        bs_doc ,bs_dlen,bs_slen,bs_label = [],[],[],[]\n",
    "        for c_data in col_datasets:\n",
    "            bs_doc.append(c_data[0])\n",
    "            bs_dlen.append(c_data[1])\n",
    "            bs_slen.append(c_data[2])\n",
    "            bs_label.append(c_data[3])\n",
    "        self.max_sent_len = min( max([lens for slen in bs_slen for lens in slen ]) ,self.word_limit )\n",
    "        self.max_doc_len = min( self.sentence_limit , max(bs_dlen))\n",
    "        pad_docs = []\n",
    "        pad_labels = []\n",
    "        for doc,label in zip(bs_doc,bs_label):\n",
    "            pad_doc = []\n",
    "            for sent in doc:\n",
    "                if len(sent) > self.max_sent_len:\n",
    "                    pad_doc.append(sent[:self.max_sent_len])\n",
    "                else:\n",
    "                    pad_doc.append(sent + (self.max_sent_len-len(sent))*[self.word_pad_idx])\n",
    "            if len(pad_doc) > self.max_doc_len:\n",
    "                pad_doc = pad_doc[:self.max_doc_len]\n",
    "            else:\n",
    "                pad_doc.extend((self.max_doc_len-len(pad_doc)) * [[self.word_pad_idx] * self.max_sent_len])\n",
    "            pad_docs.append(pad_doc)\n",
    "            pad_labels.append(self.one_hot(label))\n",
    "        bs_slen_p = []\n",
    "        for b_s in list(map(self.turncut_dlen ,bs_slen)):\n",
    "            bs_slen_p.append(list(map(self.max_slen ,b_s)))\n",
    "        bs_dlen2 = list(map(self.max_dlen,bs_dlen))\n",
    "        return torch.LongTensor(pad_docs) , torch.LongTensor(bs_dlen2), torch.LongTensor(bs_slen_p), torch.tensor(pad_labels)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "objective-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "\n",
    "class HierarchialAttentionNetwork(nn.Module):\n",
    "    def __init__(self, n_classes, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers,\n",
    "                 sentence_rnn_layers, word_att_size, sentence_att_size, dropout=0.5):\n",
    "        super(HierarchialAttentionNetwork, self).__init__()\n",
    "        self.sentence_attention = SentenceAttention(vocab_size, emb_size, word_rnn_size, sentence_rnn_size,\n",
    "                                                    word_rnn_layers, sentence_rnn_layers, word_att_size,\n",
    "                                                    sentence_att_size, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2 * sentence_rnn_size, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        document_embeddings, word_alphas, sentence_alphas = self.sentence_attention(documents, sentences_per_document,\n",
    "                                                                                    words_per_sentence)  \n",
    "        scores = self.sigmoid(self.fc(self.dropout(document_embeddings)) ) \n",
    "        \n",
    "        return scores, word_alphas, sentence_alphas\n",
    "\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers, sentence_rnn_layers,\n",
    "                 word_att_size, sentence_att_size, dropout):\n",
    "        super(SentenceAttention, self).__init__()\n",
    "\n",
    "        self.word_attention = WordAttention(vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size,\n",
    "                                            dropout)\n",
    "\n",
    "        self.sentence_rnn = nn.GRU(2 * word_rnn_size, sentence_rnn_size, num_layers=sentence_rnn_layers,\n",
    "                                   bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.sentence_attention = nn.Linear(2 * sentence_rnn_size, sentence_att_size)\n",
    "\n",
    "        self.sentence_context_vector = nn.Linear(sentence_att_size, 1,\n",
    "                                                 bias=False) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        packed_sentences = pack_padded_sequence(documents,\n",
    "                                                lengths=sentences_per_document.tolist(),\n",
    "                                                batch_first=True,\n",
    "                                                enforce_sorted=False) \n",
    "        packed_words_per_sentence = pack_padded_sequence(words_per_sentence,\n",
    "                                                         lengths=sentences_per_document.tolist(),\n",
    "                                                         batch_first=True,\n",
    "                                                         enforce_sorted=False)  \n",
    "        sentences, word_alphas = self.word_attention(packed_sentences.data,\n",
    "                                                     packed_words_per_sentence.data)  \n",
    "        \n",
    "        sentences = self.dropout(sentences)\n",
    "        \n",
    "        packed_sentences, _ = self.sentence_rnn(PackedSequence(data=sentences,\n",
    "                                                               batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                               sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                               unsorted_indices=packed_sentences.unsorted_indices))  \n",
    "        att_s = self.sentence_attention(packed_sentences.data)  \n",
    "        att_s = torch.tanh(att_s)  \n",
    "        \n",
    "        att_s = self.sentence_context_vector(att_s).squeeze(1)  \n",
    "\n",
    "        max_value = att_s.max()  \n",
    "        att_s = torch.exp(att_s - max_value) \n",
    "\n",
    "        att_s, _ = pad_packed_sequence(PackedSequence(data=att_s,\n",
    "                                                      batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                      sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                      unsorted_indices=packed_sentences.unsorted_indices),\n",
    "                                       batch_first=True)  # (n_documents, max(sentences_per_document))\n",
    "\n",
    "  \n",
    "        sentence_alphas = att_s / torch.sum(att_s, dim=1, keepdim=True)\n",
    "\n",
    "        documents, _ = pad_packed_sequence(packed_sentences,\n",
    "                                           batch_first=True)  \n",
    "        documents = documents * sentence_alphas.unsqueeze(2)  \n",
    "        documents = documents.sum(dim=1)\n",
    "        word_alphas, _ = pad_packed_sequence(PackedSequence(data=word_alphas,\n",
    "                                                            batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                            sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                            unsorted_indices=packed_sentences.unsorted_indices),\n",
    "                                             batch_first=True) \n",
    "        return documents, word_alphas, sentence_alphas\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size, dropout):\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.word_rnn = nn.GRU(emb_size, word_rnn_size, num_layers=word_rnn_layers, bidirectional=True,\n",
    "                               dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.word_attention = nn.Linear(2 * word_rnn_size, word_att_size)\n",
    "        self.word_context_vector = nn.Linear(word_att_size, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def init_embeddings(self, embeddings):\n",
    "        self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=False):\n",
    "        for p in self.embeddings.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def forward(self, sentences, words_per_sentence):\n",
    "        sentences = self.dropout(self.embeddings(sentences))\n",
    "        packed_words = pack_padded_sequence(sentences,\n",
    "                                            lengths=words_per_sentence.tolist(),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False) \n",
    "        # a PackedSequence object, where 'data' is the flattened words (n_words, word_emb)\n",
    "\n",
    "        packed_words, _ = self.word_rnn(packed_words)  \n",
    "        att_w = self.word_attention(packed_words.data)\n",
    "        att_w = torch.tanh(att_w) \n",
    "        att_w = self.word_context_vector(att_w).squeeze(1)  # (n_words)\n",
    "\n",
    "        max_value = att_w.max()  \n",
    "        att_w = torch.exp(att_w - max_value)  \n",
    "        \n",
    "        att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,batch_sizes=packed_words.batch_sizes,\n",
    "                                    sorted_indices=packed_words.sorted_indices,\n",
    "                                    unsorted_indices=packed_words.unsorted_indices),\n",
    "                                       batch_first=True)\n",
    "        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True) \n",
    "\n",
    "        sentences, _ = pad_packed_sequence(packed_words,\n",
    "                                           batch_first=True)  \n",
    "\n",
    "        sentences = sentences * word_alphas.unsqueeze(2) \n",
    "        sentences = sentences.sum(dim=1) \n",
    "\n",
    "        return sentences, word_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "assured-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tag2idx) \n",
    "word_rnn_size = 100  ## 超參數設定 可以調整這裡的參數\n",
    "sentence_rnn_size = 100 \n",
    "word_rnn_layers = 2  \n",
    "sentence_rnn_layers = 2  \n",
    "word_att_size = 200\n",
    "sentence_att_size = 200  \n",
    "dropout = 0.3\n",
    "\n",
    "BS = 16\n",
    "lr = 1e-3\n",
    "\n",
    "## 到這裡之前\n",
    "\n",
    "PAD_IDX = word2idx['<pad>']\n",
    "dataset = HANDataset(dict_han,word2idx['<pad>'],len(tag2idx))\n",
    "test_loader = DataLoader(dataset, batch_size=1,\n",
    "                        collate_fn=dataset.collate_fn)\n",
    "model = HierarchialAttentionNetwork(n_classes=n_classes,\n",
    "                                    vocab_size=len(word2idx),\n",
    "                                    emb_size=300,\n",
    "                                    word_rnn_size=word_rnn_size,\n",
    "                                    sentence_rnn_size=sentence_rnn_size,\n",
    "                                    word_rnn_layers=word_rnn_layers,\n",
    "                                    sentence_rnn_layers=sentence_rnn_layers,\n",
    "                                    word_att_size=word_att_size,\n",
    "                                    sentence_att_size=sentence_att_size,\n",
    "                                    dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "starting-science",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchialAttentionNetwork(\n",
       "  (sentence_attention): SentenceAttention(\n",
       "    (word_attention): WordAttention(\n",
       "      (embeddings): Embedding(37743, 300)\n",
       "      (word_rnn): GRU(300, 100, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      (word_attention): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (word_context_vector): Linear(in_features=200, out_features=1, bias=False)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (sentence_rnn): GRU(200, 100, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (sentence_attention): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (sentence_context_vector): Linear(in_features=200, out_features=1, bias=False)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=200, out_features=484, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('legaltech_han_model.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "vertical-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement : \n",
      "torch.Size([1, 55, 35])\n",
      "tensor([55], device='cuda:0')\n",
      "tensor([[20, 15,  2,  8,  8, 17,  5,  6,  6, 10,  7, 25,  7, 20, 11,  6, 18, 12,\n",
      "          2,  3,  5,  4, 26,  1, 14, 14, 13, 21,  3,  8,  6,  6, 18, 15, 35, 12,\n",
      "         15,  9, 11,  4,  5, 14, 13, 12,  5, 12, 13, 10,  6,  3,  6,  3, 10,  3,\n",
      "          7]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "    for test_data in test_loader:\n",
    "        documents = test_data[0].to(device)\n",
    "        print(\"Judgement : \")\n",
    "        for sent in documents[0]:\n",
    "            for word in sent:\n",
    "                if word == 0:\n",
    "                    continue\n",
    "#                 print(idx2word[word.cpu().item()],end = '')\n",
    "#             print()\n",
    "        sentences_per_document = test_data[1].to(device)\n",
    "        words_per_sentence = test_data[2].to(device)\n",
    "        print(documents.shape)\n",
    "        print(sentences_per_document)\n",
    "        print(words_per_sentence)\n",
    "        scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n",
    "                                                                 words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "armed-jamaica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET /static/css/nicepage.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET /static/css/Page-1.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET /static/css/bootstrap/bootstrap.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET /static/images/desktop_top-banner.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [11/Sep/2021 11:14:14] \"GET /static/images/banner-bg.png HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://7c82-140-127-114-22.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE = 'templates'\n",
    "STATIC = 'static'\n",
    "\n",
    "app = Flask(__name__,template_folder=TEMPLATE,static_folder=STATIC)\n",
    "run_with_ngrok(app)\n",
    "input_text = ''\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('form.html')\n",
    "@app.route('/submit', methods=['POST'])\n",
    "def submit():\n",
    "    for test_data in test_loader:\n",
    "        documents = test_data[0].to(device)\n",
    "        print(\"Judgement : \")\n",
    "        for sent in documents[0]:\n",
    "            for word in sent:\n",
    "                if word == 0:\n",
    "                    continue\n",
    "#                 print(idx2word[word.cpu().item()],end = '')\n",
    "#             print()\n",
    "        sentences_per_document = test_data[1].to(device)\n",
    "        words_per_sentence = test_data[2].to(device)\n",
    "        print(documents)\n",
    "        scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n",
    "                                                                 words_per_sentence)\n",
    "    print('\\nPredict lawNames : ')\n",
    "    predicts = ''\n",
    "    for idx,s in enumerate(scores[0]):\n",
    "        if s>0.5:\n",
    "            print(idx2tag[idx])\n",
    "            predicts += idx2tag[idx] + '\\n'\n",
    "    print('\\nTrue lawNames : ')\n",
    "    anwsers = []\n",
    "    for idx,s in enumerate(test_data[3][0]):\n",
    "        if s != 0:\n",
    "            print(idx2tag[idx])\n",
    "            anwsers.append(idx2tag[idx])\n",
    "    input_text = request.form['text']\n",
    "    if len(input_text) < 100:\n",
    "        vis2 = 1\n",
    "        s = 'Unable to send your message. Please fix errors then try again.'\n",
    "        return render_template('form.html',vis2=vis2,input_text=input_text,s = s)\n",
    "    elif len(input_text) > 100:\n",
    "        vis1 = 1\n",
    "        return render_template('form.html',vis1=vis1,input_text=input_text,s = predicts)\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "integral-johnson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement : \n",
      "福建金門地方法院刑事裁定104年度金訴字第1號上訴人即被告蔡銀英上列被告因違反銀行法案件\n",
      "不服本院民國104年12月31日104年度金訴字第1號第一審刑事判決\n",
      "提起上訴\n",
      "本院裁定如下：主文上訴駁回\n",
      "理由一、按上訴期間為10日\n",
      "自送達判決後起算；原審法院認為上訴不合法律上之程式者\n",
      "應以裁定駁回之\n",
      "刑事訴訟法第349條前段\n",
      "第362條前段定有明文\n",
      "次按送達於在監獄或看守所之人\n",
      "應囑託該監所長官為之\n",
      "刑事訴訟法第56條第2項亦定有明文；又送達於應受送達人之住、居所、事務所或營業所\n",
      "不獲會晤應受送達人\n",
      "亦無法準用民事訴訟法第137條規定將文書付與有辨別事理能力之同居人或受僱人者\n",
      "得將文書寄存送達地之自治或警察機關\n",
      "並作送達通知書2份\n",
      "1份黏貼於應受送達人住居所、事務所、營業所或其就業處所門首\n",
      "另一份置於該送達處所信箱或其他適當位置\n",
      "以為送達\n",
      "又寄存送達\n",
      "自寄存之日起\n",
      "經10日發生效力\n",
      "刑事訴訟法第62條準用民事訴訟法第136條至138條規定甚明；而民事訴訟法第138條第2項所稱之\n",
      "係指\n",
      "二、經查：本件上訴人即被告蔡銀英違反銀行法案件\n",
      "其於本院審理時陳明其住所為金門縣○○鎮○○路00號\n",
      "有本院歷次準備程序筆錄、審判筆錄附卷可稽\n",
      "而本院於民國104年12月31日以104年度金訴字第1號刑事判決判處有期徒刑1年8月\n",
      "緩刑3年\n",
      "並應於判決確定1年內\n",
      "向公庫支付新臺幣8萬元\n",
      "犯罪所得新臺幣<unk>元沒收\n",
      "該判決於105年1月6日寄存送達被告住所所在之金門縣警察局金湖分局金湖派出所\n",
      "被告本人於105年1月7日15時14分許前往該警局領取等情\n",
      "有個人戶籍資料查詢結果、本院送達證書、入出境資訊連結作業、金門縣警察局金湖分局金湖派出所受理司法文書寄存登記簿各1份在卷可查\n",
      "又被告於斯時並未因案在監執行或羈押\n",
      "此有臺灣高等法院在監在押全國紀錄表1份附卷可據\n",
      "是被告並非在監獄或看守所之人\n",
      "其主張該判決應囑託監所長官為送達云云\n",
      "顯非可採\n",
      "則依上開送達情形\n",
      "該判決正本既已經被告於105年1月7日前往金湖派出所具領\n",
      "是本件上訴期間應自105年1月8日起算10日\n",
      "再依法院訴訟當事人在途期間標準第2條之規定\n",
      "加計在途期間1日\n",
      "是其上訴期間之末日應為105年1月18日屆滿\n",
      "詎上訴人遲至107年3月13日始向本院提起上訴\n",
      "有刑事上訴狀上本院收狀戳章為憑\n",
      "顯已逾越法定上訴期間\n",
      "且無從補正\n",
      "揆諸首揭法律規定\n",
      "應予駁回\n",
      "三、應依刑事訴訟法第362條前段\n",
      "裁定如主文\n",
      "中華民國107年3月28日刑事第二庭\n",
      "\n",
      "Predict lawNames : \n",
      "刑事訴訟法62\n",
      "刑事訴訟法362\n",
      "民事訴訟法137\n",
      "刑事訴訟法349\n",
      "\n",
      "True lawNames : \n",
      "刑事訴訟法62\n",
      "刑事訴訟法362\n",
      "民事訴訟法137\n",
      "民事訴訟法138 2\n",
      "刑事訴訟法349\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-courage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
