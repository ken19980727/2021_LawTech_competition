{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "herbal-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fin_word.json', 'r', encoding='utf-8') as f:\n",
    "    word2idx = json.load(f)\n",
    "with open('fin_tag.json', 'r', encoding='utf-8') as f:\n",
    "    tag2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serial-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['<pad>'] = 0\n",
    "idx2word = {word2idx[w]:w for w in word2idx}\n",
    "idx2tag = {tag2idx[t]:t for t in tag2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "better-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_limit = 40 ## 每句話的最長長度\n",
    "sentence_limit = 80 ## 每篇判決的句子數目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prompt-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader ,SubsetRandomSampler ,ConcatDataset ,Dataset\n",
    "class HANDataset(Dataset):\n",
    "    def __init__(self,word_pad_idx,num_classes , word_limit = 40 ,sentence_limit = 80 ):\n",
    "        # Load data\n",
    "        self.data = torch.load('FIN_data.pth.tar')\n",
    "        self.word_limit = word_limit\n",
    "        self.sentence_limit = sentence_limit\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __getitem__(self, id_doc):\n",
    "        return (self.data['docs'][id_doc], \\\n",
    "               self.data['sentences_per_document'][id_doc], \\\n",
    "               self.data['words_per_sentence'][id_doc], \\\n",
    "               self.data['labels'][id_doc])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['labels'])\n",
    "    \n",
    "    def one_hot(self,tag):\n",
    "        bs_oh_label = torch.tensor([0.0] * self.num_classes)\n",
    "        for t in tag:\n",
    "            bs_oh_label += torch.eye(self.num_classes)[t]\n",
    "        return bs_oh_label.tolist()\n",
    "    \n",
    "    def turncut_dlen(self,x):\n",
    "        if len(x) > self.max_doc_len:\n",
    "            return x[:self.max_doc_len]\n",
    "        elif len(x) < self.max_doc_len:\n",
    "            return x + [0] * (self.max_doc_len-len(x))\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def max_slen(self,x):\n",
    "        if x > self.max_sent_len:\n",
    "            return self.max_sent_len\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def max_dlen(self,x):\n",
    "        if x > self.max_doc_len:\n",
    "            return self.max_doc_len\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def collate_fn(self, col_datasets):\n",
    "        bs_doc ,bs_dlen,bs_slen,bs_label = [],[],[],[]\n",
    "        for c_data in col_datasets:\n",
    "            bs_doc.append(c_data[0])\n",
    "            bs_dlen.append(c_data[1])\n",
    "            bs_slen.append(c_data[2])\n",
    "            bs_label.append(c_data[3])\n",
    "        self.max_sent_len = min( max([lens for slen in bs_slen for lens in slen ]) ,self.word_limit )\n",
    "        self.max_doc_len = min( self.sentence_limit , max(bs_dlen))\n",
    "        pad_docs = []\n",
    "        pad_labels = []\n",
    "        for doc,label in zip(bs_doc,bs_label):\n",
    "            pad_doc = []\n",
    "            for sent in doc:\n",
    "                if len(sent) > self.max_sent_len:\n",
    "                    pad_doc.append(sent[:self.max_sent_len])\n",
    "                else:\n",
    "                    pad_doc.append(sent + (self.max_sent_len-len(sent))*[self.word_pad_idx])\n",
    "            if len(pad_doc) > self.max_doc_len:\n",
    "                pad_doc = pad_doc[:self.max_doc_len]\n",
    "            else:\n",
    "                pad_doc.extend((self.max_doc_len-len(pad_doc)) * [[self.word_pad_idx] * self.max_sent_len])\n",
    "            pad_docs.append(pad_doc)\n",
    "            pad_labels.append(self.one_hot(label))\n",
    "        bs_slen_p = []\n",
    "        for b_s in list(map(self.turncut_dlen ,bs_slen)):\n",
    "            bs_slen_p.append(list(map(self.max_slen ,b_s)))\n",
    "        bs_dlen2 = list(map(self.max_dlen,bs_dlen))\n",
    "        return torch.LongTensor(pad_docs) , torch.LongTensor(bs_dlen2), torch.LongTensor(bs_slen_p), torch.tensor(pad_labels)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entertaining-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "\n",
    "class HierarchialAttentionNetwork(nn.Module):\n",
    "    def __init__(self, n_classes, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers,\n",
    "                 sentence_rnn_layers, word_att_size, sentence_att_size, dropout=0.5):\n",
    "        super(HierarchialAttentionNetwork, self).__init__()\n",
    "        self.sentence_attention = SentenceAttention(vocab_size, emb_size, word_rnn_size, sentence_rnn_size,\n",
    "                                                    word_rnn_layers, sentence_rnn_layers, word_att_size,\n",
    "                                                    sentence_att_size, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2 * sentence_rnn_size, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        document_embeddings, word_alphas, sentence_alphas = self.sentence_attention(documents, sentences_per_document,\n",
    "                                                                                    words_per_sentence)  \n",
    "        scores = self.sigmoid(self.fc(self.dropout(document_embeddings)) ) \n",
    "        \n",
    "        return scores, word_alphas, sentence_alphas\n",
    "\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers, sentence_rnn_layers,\n",
    "                 word_att_size, sentence_att_size, dropout):\n",
    "        super(SentenceAttention, self).__init__()\n",
    "\n",
    "        self.word_attention = WordAttention(vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size,\n",
    "                                            dropout)\n",
    "\n",
    "        self.sentence_rnn = nn.GRU(2 * word_rnn_size, sentence_rnn_size, num_layers=sentence_rnn_layers,\n",
    "                                   bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.sentence_attention = nn.Linear(2 * sentence_rnn_size, sentence_att_size)\n",
    "\n",
    "        self.sentence_context_vector = nn.Linear(sentence_att_size, 1,\n",
    "                                                 bias=False) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        packed_sentences = pack_padded_sequence(documents,\n",
    "                                                lengths=sentences_per_document.tolist(),\n",
    "                                                batch_first=True,\n",
    "                                                enforce_sorted=False) \n",
    "        packed_words_per_sentence = pack_padded_sequence(words_per_sentence,\n",
    "                                                         lengths=sentences_per_document.tolist(),\n",
    "                                                         batch_first=True,\n",
    "                                                         enforce_sorted=False)  \n",
    "        sentences, word_alphas = self.word_attention(packed_sentences.data,\n",
    "                                                     packed_words_per_sentence.data)  \n",
    "        \n",
    "        sentences = self.dropout(sentences)\n",
    "        \n",
    "        packed_sentences, _ = self.sentence_rnn(PackedSequence(data=sentences,\n",
    "                                                               batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                               sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                               unsorted_indices=packed_sentences.unsorted_indices))  \n",
    "        att_s = self.sentence_attention(packed_sentences.data)  \n",
    "        att_s = torch.tanh(att_s)  \n",
    "        \n",
    "        att_s = self.sentence_context_vector(att_s).squeeze(1)  \n",
    "\n",
    "        max_value = att_s.max()  \n",
    "        att_s = torch.exp(att_s - max_value) \n",
    "\n",
    "        att_s, _ = pad_packed_sequence(PackedSequence(data=att_s,\n",
    "                                                      batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                      sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                      unsorted_indices=packed_sentences.unsorted_indices),\n",
    "                                       batch_first=True)  # (n_documents, max(sentences_per_document))\n",
    "\n",
    "  \n",
    "        sentence_alphas = att_s / torch.sum(att_s, dim=1, keepdim=True)\n",
    "\n",
    "        documents, _ = pad_packed_sequence(packed_sentences,\n",
    "                                           batch_first=True)  \n",
    "        documents = documents * sentence_alphas.unsqueeze(2)  \n",
    "        documents = documents.sum(dim=1)\n",
    "        word_alphas, _ = pad_packed_sequence(PackedSequence(data=word_alphas,\n",
    "                                                            batch_sizes=packed_sentences.batch_sizes,\n",
    "                                                            sorted_indices=packed_sentences.sorted_indices,\n",
    "                                                            unsorted_indices=packed_sentences.unsorted_indices),\n",
    "                                             batch_first=True) \n",
    "        return documents, word_alphas, sentence_alphas\n",
    "\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size, dropout):\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.word_rnn = nn.GRU(emb_size, word_rnn_size, num_layers=word_rnn_layers, bidirectional=True,\n",
    "                               dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.word_attention = nn.Linear(2 * word_rnn_size, word_att_size)\n",
    "        self.word_context_vector = nn.Linear(word_att_size, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def init_embeddings(self, embeddings):\n",
    "        self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=False):\n",
    "        for p in self.embeddings.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def forward(self, sentences, words_per_sentence):\n",
    "        sentences = self.dropout(self.embeddings(sentences))\n",
    "        packed_words = pack_padded_sequence(sentences,\n",
    "                                            lengths=words_per_sentence.tolist(),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False) \n",
    "        # a PackedSequence object, where 'data' is the flattened words (n_words, word_emb)\n",
    "\n",
    "        packed_words, _ = self.word_rnn(packed_words)  \n",
    "        att_w = self.word_attention(packed_words.data)\n",
    "        att_w = torch.tanh(att_w) \n",
    "        att_w = self.word_context_vector(att_w).squeeze(1)  # (n_words)\n",
    "\n",
    "        max_value = att_w.max()  \n",
    "        att_w = torch.exp(att_w - max_value)  \n",
    "        \n",
    "        att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,batch_sizes=packed_words.batch_sizes,\n",
    "                                    sorted_indices=packed_words.sorted_indices,\n",
    "                                    unsorted_indices=packed_words.unsorted_indices),\n",
    "                                       batch_first=True)\n",
    "        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True) \n",
    "\n",
    "        sentences, _ = pad_packed_sequence(packed_words,\n",
    "                                           batch_first=True)  \n",
    "\n",
    "        sentences = sentences * word_alphas.unsqueeze(2) \n",
    "        sentences = sentences.sum(dim=1) \n",
    "\n",
    "        return sentences, word_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "constitutional-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tag2idx) \n",
    "word_rnn_size = 100  ## 超參數設定 可以調整這裡的參數\n",
    "sentence_rnn_size = 100 \n",
    "word_rnn_layers = 1  \n",
    "sentence_rnn_layers = 1  \n",
    "word_att_size = 200\n",
    "sentence_att_size = 200  \n",
    "dropout = 0\n",
    "\n",
    "BS = 16\n",
    "lr = 1e-4\n",
    "epochs = 50\n",
    "\n",
    "## 到這裡之前\n",
    "\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "PAD_IDX = word2idx['<pad>']\n",
    "datasets = HANDataset(PAD_IDX,len(tag2idx))\n",
    "dataloader = DataLoader(datasets , batch_size=BS, collate_fn =  datasets.collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cloudy-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (documents, sentences_per_document, words_per_sentence, labels) in enumerate(dataloader):\n",
    "#     if i == 70:\n",
    "#         print(documents)\n",
    "#         print(sentences_per_document)\n",
    "#         print(words_per_sentence)\n",
    "#         print(labels)\n",
    "#         print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optional-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "# class F1():\n",
    "#     def __init__(self):\n",
    "#         self.threshold = 0.5\n",
    "#         self.n_precision = 0\n",
    "#         self.n_recall = 0\n",
    "#         self.n_corrects = 0\n",
    "#         self.name = 'F1'\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.n_precision = 0\n",
    "#         self.n_recall = 0\n",
    "#         self.n_corrects = 0\n",
    "\n",
    "#     def update(self, predicts, groundTruth):\n",
    "#         predicts = (predicts > self.threshold).float()\n",
    "#         self.n_precision += torch.sum(predicts).data.item()\n",
    "#         self.n_recall += torch.sum(groundTruth).data.item()\n",
    "#         self.n_corrects += torch.sum(groundTruth * predicts).data.item()\n",
    "\n",
    "#     def get_score(self):\n",
    "#         recall = self.n_corrects / self.n_recall\n",
    "#         precision = self.n_corrects / (self.n_precision + 1e-20) #prevent divided by zero\n",
    "#         return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "#     def print_score(self):\n",
    "#         score = self.get_score()\n",
    "#         return '{:.5f}'.format(score)\n",
    "    \n",
    "def threshold_sc(x):\n",
    "    if x > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stuffed-dayton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.002756409024063779 loss: 0.18030077939455932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:16<13:51, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.041356902972266484\n",
      "train accuracy: 0.0 loss: 0.03654604209950557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:32<12:42, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03581831110893069\n",
      "train accuracy: 0.0 loss: 0.03416315551746536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:46<12:01, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03492819009399092\n",
      "train accuracy: 0.0 loss: 0.03363122318958019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:01<11:37, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03464566276886979\n",
      "train accuracy: 0.0 loss: 0.03344333081788666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:16<11:12, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03450981210413817\n",
      "train accuracy: 0.0 loss: 0.03315043503659087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:30<10:51, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03387723576176811\n",
      "train accuracy: 0.0 loss: 0.03234838444303285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:45<10:35, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.03249565798889946\n",
      "train accuracy: 0.0 loss: 0.030634173832542903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:59<10:15, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.0 loss: 0.030550286777921626\n",
      "train accuracy: 0.03734249671447527 loss: 0.028719280893961946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:14<09:56, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.06971234852196904 loss: 0.028572420942018163\n",
      "train accuracy: 0.12039023752476838 loss: 0.02674137187601478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:28<09:38, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.17014961320120053 loss: 0.026715691931344366\n",
      "train accuracy: 0.22457802183967882 loss: 0.025119364096618094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:43<09:26, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.2261207739446171 loss: 0.025295998699761724\n",
      "train accuracy: 0.2663692826710735 loss: 0.02375819669147064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:57<09:07, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.2527882417465354 loss: 0.024038178367993317\n",
      "train accuracy: 0.3025684002546468 loss: 0.022571980508531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [03:11<08:52, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.304912973403515 loss: 0.02292974806717924\n",
      "train accuracy: 0.3430004845967391 loss: 0.021537870336491783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:26<08:40, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.3237561684990407 loss: 0.021990278005801344\n",
      "train accuracy: 0.3711861643759149 loss: 0.020634245155407144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:40<08:27, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.36200996866868496 loss: 0.0211263294205875\n",
      "train accuracy: 0.39257517285420324 loss: 0.019837347881493717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:55<08:13, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.3793881216198733 loss: 0.020381920064824657\n",
      "train accuracy: 0.41055671502225255 loss: 0.019131986116682802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [04:09<07:58, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.4091200131060598 loss: 0.01971372657430333\n",
      "train accuracy: 0.43887692932055933 loss: 0.01849037419176764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [04:24<07:43, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.44152563902985664 loss: 0.019048277137650026\n",
      "train accuracy: 0.4693367267583461 loss: 0.01789571977760251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:38<07:28, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.4724395378235945 loss: 0.01847409663369527\n",
      "train accuracy: 0.49061309227866434 loss: 0.01739498427523686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [04:53<07:14, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.4766148892527336 loss: 0.01796260060792839\n",
      "train accuracy: 0.4991674461345462 loss: 0.016973765754328296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [05:07<06:58, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.4867141779998455 loss: 0.017588371108915354\n",
      "train accuracy: 0.5132028832791932 loss: 0.016621732126429183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [05:21<06:43, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5035037569870384 loss: 0.01725112347284684\n",
      "train accuracy: 0.5258282116483635 loss: 0.016324953292146907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:36<06:26, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5255180726303548 loss: 0.016865343736434304\n",
      "train accuracy: 0.5347001095260254 loss: 0.015994510073606315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [05:50<06:12, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5297614125026106 loss: 0.01655903398185163\n",
      "train accuracy: 0.5432649748237778 loss: 0.015693097635432408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [06:05<06:01, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5346575179225919 loss: 0.016213379775148792\n",
      "train accuracy: 0.5440356072729813 loss: 0.01541774039500737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [06:19<05:49, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5417285964647067 loss: 0.01595191707884943\n",
      "train accuracy: 0.5494414440269386 loss: 0.015149254848280022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [06:34<05:34, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.547875464078075 loss: 0.01563630874797299\n",
      "train accuracy: 0.5576279705014233 loss: 0.014890899397880287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [06:48<05:18, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5515680338148867 loss: 0.015453184338135494\n",
      "train accuracy: 0.5631035277392679 loss: 0.014641320865668423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [07:02<05:02, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5554095632172942 loss: 0.015170267820861694\n",
      "train accuracy: 0.5679622900819676 loss: 0.014421355958144227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [07:17<04:48, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5653192903304468 loss: 0.014922125113976968\n",
      "train accuracy: 0.5822263671308947 loss: 0.014204263923460388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [07:31<04:34, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5694934255295037 loss: 0.014636147893159776\n",
      "train accuracy: 0.5854070295909248 loss: 0.013992803741817002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [07:46<04:23, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5782275661587425 loss: 0.014372530720523885\n",
      "train accuracy: 0.5929708105938364 loss: 0.013797701360215922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [08:02<04:11, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid accuracy: 0.5843004325774908 loss: 0.014187120530452277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [08:02<04:08, 14.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8ffeea2afd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                                              words_per_sentence)\n\u001b[0;32m     44\u001b[0m                 \u001b[0mbs_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mbs_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbs_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(datasets)):\n",
    "#     print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "#     print(valid_ids)\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_dataloader = DataLoader(datasets, batch_size=BS,\n",
    "                                  collate_fn=datasets.collate_fn,\n",
    "                                  sampler=train_subsampler)\n",
    "    valid_dataloader = DataLoader(datasets, batch_size=BS,\n",
    "                                  collate_fn=datasets.collate_fn,\n",
    "                                  sampler=valid_subsampler)\n",
    "    model = HierarchialAttentionNetwork(n_classes=n_classes,\n",
    "                                        vocab_size=len(word2idx),\n",
    "                                        emb_size=300,\n",
    "                                        word_rnn_size=word_rnn_size,\n",
    "                                        sentence_rnn_size=sentence_rnn_size,\n",
    "                                        word_rnn_layers=word_rnn_layers,\n",
    "                                        sentence_rnn_layers=sentence_rnn_layers,\n",
    "                                        word_att_size=word_att_size,\n",
    "                                        sentence_att_size=sentence_att_size,\n",
    "                                        dropout=dropout)\n",
    "    model.sentence_attention.word_attention.fine_tune_embeddings(True)\n",
    "    optimizer = optim.AdamW(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "#     criterion = BCEFocalLosswithLogits() ## 可以避免計算過多背景部分(答案為0) \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    all_loader = {\n",
    "        'train' : train_dataloader,\n",
    "        'valid' : valid_dataloader,\n",
    "    }\n",
    "    for epoch in trange(epochs):\n",
    "        for loader in all_loader:\n",
    "            loss = 0\n",
    "            accuracy = []\n",
    "            for i, (documents, sentences_per_document, words_per_sentence, labels) in enumerate(all_loader[loader]):\n",
    "                documents = documents.to(device)\n",
    "                sentences_per_document = sentences_per_document.to(device)  \n",
    "                words_per_sentence = words_per_sentence.to(device)\n",
    "                labels = labels.to(device)\n",
    "                scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n",
    "                                                             words_per_sentence)\n",
    "                bs_loss = criterion(scores ,labels)\n",
    "                bs_loss.backward()\n",
    "                optimizer.step()\n",
    "                loss += bs_loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                y_true = [list(map(lambda s:int(s),label)) for label in labels.cpu().tolist()]\n",
    "                y_pred = [list(map(threshold_sc,sc)) for sc in scores.cpu().tolist()]\n",
    "                bs_acc = f1_score(y_true,y_pred,average = 'micro')\n",
    "                accuracy.append(bs_acc)\n",
    "            print(f'{loader} accuracy:',np.mean(np.array(accuracy)), 'loss:',loss/ len(all_loader[loader]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-astrology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  31,   39,   55,   65,   68,   70,  114,  121,  124,  129,  137,\n",
       "        138,  143,  166,  175,  187,  188,  195,  199,  207,  211,  225,\n",
       "        226,  230,  231,  237,  246,  250,  252,  256,  257,  275,  300,\n",
       "        305,  312,  313,  320,  326,  331,  380,  381,  386,  392,  395,\n",
       "        435,  437,  442,  446,  471,  476,  492,  498,  510,  527,  532,\n",
       "        548,  551,  554,  557,  564,  591,  622,  630,  632,  638,  648,\n",
       "        666,  668,  688,  691,  700,  701,  702,  720,  728,  747,  767,\n",
       "        786,  792,  798,  800,  816,  820,  834,  843,  851,  857,  870,\n",
       "        880,  889,  912,  917,  973,  974,  976,  984,  995, 1003, 1015,\n",
       "       1037, 1050, 1058, 1074, 1080, 1086, 1096, 1105, 1108, 1145, 1151,\n",
       "       1155, 1182, 1196, 1202, 1203, 1205, 1210, 1225, 1245, 1261, 1276,\n",
       "       1287, 1292, 1293, 1314, 1315, 1318, 1323, 1325, 1347, 1361, 1362,\n",
       "       1372, 1381, 1387, 1395, 1402, 1410, 1413, 1443, 1445, 1455, 1461,\n",
       "       1465, 1467, 1469, 1480, 1499, 1519, 1532, 1541, 1547, 1578, 1584,\n",
       "       1587, 1601, 1609, 1619, 1629, 1630, 1637, 1638, 1651, 1653, 1659,\n",
       "       1676, 1680, 1692, 1695, 1704, 1715, 1718, 1719, 1720, 1722, 1723,\n",
       "       1725, 1729, 1745, 1749, 1751, 1765, 1768, 1778, 1781, 1798, 1800,\n",
       "       1808, 1812, 1836, 1842, 1843, 1846, 1882, 1893, 1897, 1898, 1931,\n",
       "       1935, 1941, 1963, 1965, 1982, 2004, 2025, 2040, 2041, 2052, 2063,\n",
       "       2087, 2104, 2111, 2114, 2118, 2122, 2128, 2133, 2147, 2149, 2154,\n",
       "       2157, 2162, 2178, 2219, 2224, 2250, 2256, 2257, 2291, 2299, 2301,\n",
       "       2332, 2340, 2386, 2395, 2406, 2432, 2454, 2460, 2482, 2491, 2504,\n",
       "       2518, 2520, 2522, 2525, 2530, 2546, 2558, 2563, 2576, 2583, 2596,\n",
       "       2600, 2609, 2626, 2627, 2640, 2659, 2663, 2664, 2674, 2679, 2707,\n",
       "       2709, 2720, 2754, 2756, 2759, 2765, 2804, 2805, 2812, 2838, 2857,\n",
       "       2869, 2871, 2884, 2896, 2897, 2903, 2906, 2910, 2923, 2930, 2933,\n",
       "       2934, 2939, 2941, 2951, 2955, 2961, 2972, 2977, 2983, 2984, 2996,\n",
       "       3001, 3014, 3021, 3024, 3028, 3030, 3040, 3043, 3048, 3061, 3074,\n",
       "       3078, 3083, 3092, 3110, 3114, 3115, 3130, 3157, 3158, 3164, 3172,\n",
       "       3179, 3202, 3209, 3211, 3229, 3230, 3232, 3244, 3246, 3259, 3288,\n",
       "       3291, 3299, 3308, 3313, 3335, 3342, 3348, 3357, 3365, 3374, 3375,\n",
       "       3380, 3381, 3383, 3390, 3393, 3400, 3419, 3420, 3435, 3439, 3443,\n",
       "       3467, 3472, 3474, 3481, 3484, 3516, 3522, 3527, 3537, 3538, 3548,\n",
       "       3565, 3577, 3580, 3584, 3600, 3613, 3625, 3631, 3640, 3658, 3664,\n",
       "       3669, 3687, 3692, 3707, 3708, 3749, 3757, 3760, 3765, 3776, 3789,\n",
       "       3790, 3800, 3817, 3818, 3831, 3834, 3841, 3867, 3869, 3870, 3873,\n",
       "       3874, 3877, 3884, 3894, 3918, 3923, 3925, 3936, 3941, 3977, 3981,\n",
       "       3994, 3995, 4002, 4010, 4031, 4033, 4039, 4054, 4065, 4071, 4080,\n",
       "       4093, 4111, 4113, 4116, 4122, 4132, 4139, 4147, 4149, 4174, 4186,\n",
       "       4187, 4208, 4235, 4244, 4258, 4267, 4282, 4291, 4294, 4295, 4298,\n",
       "       4300, 4311, 4314, 4319, 4335, 4342, 4343, 4360, 4366, 4371, 4375,\n",
       "       4411, 4448, 4483, 4484, 4485, 4494, 4510, 4514, 4528, 4538, 4541,\n",
       "       4568, 4580, 4582, 4599, 4609, 4615, 4618, 4643, 4645, 4652, 4654,\n",
       "       4696, 4709, 4727, 4732, 4733, 4744, 4756, 4781, 4800, 4813, 4817,\n",
       "       4832, 4845, 4861, 4888, 4890, 4893, 4922, 4941, 4950, 4970, 4986,\n",
       "       4998, 4999, 5001, 5013, 5047, 5057, 5075, 5082, 5089, 5090, 5093,\n",
       "       5096, 5110, 5131, 5151, 5170, 5172, 5179, 5191, 5204, 5227, 5233,\n",
       "       5237, 5243, 5252, 5258, 5260, 5269, 5285, 5290, 5295, 5302, 5306,\n",
       "       5316, 5321, 5333, 5339, 5356, 5396, 5403, 5407, 5415, 5420, 5428,\n",
       "       5429, 5432, 5456, 5461, 5471, 5486, 5490, 5496, 5510, 5511, 5519,\n",
       "       5520, 5522, 5524, 5545, 5566, 5578, 5582, 5591, 5618, 5620, 5625,\n",
       "       5626, 5632, 5651, 5653, 5654, 5660, 5691, 5695, 5697, 5698, 5708,\n",
       "       5717, 5732, 5734, 5738, 5754, 5764, 5770, 5784, 5811, 5812, 5822,\n",
       "       5842, 5846, 5868, 5872, 5875, 5880, 5899, 5903, 5904])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-safety",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement : \n",
      "福建金門地方法院刑事裁定104年度金訴字第1號上訴人即被告蔡銀英上列被告因違反銀行法案件\n",
      "不服本院民國104年12月31日104年度金訴字第1號第一審刑事判決\n",
      "提起上訴\n",
      "本院裁定如下：主文上訴駁回\n",
      "理由一、按上訴期間為10日\n",
      "自送達判決後起算；原審法院認為上訴不合法律上之程式者\n",
      "應以裁定駁回之\n",
      "刑事訴訟法第349條前段\n",
      "第362條前段定有明文\n",
      "次按送達於在監獄或看守所之人\n",
      "應囑託該監所長官為之\n",
      "刑事訴訟法第56條第2項亦定有明文；又送達於應受送達人之住、居所、事務所或營業所\n",
      "不獲會晤應受送達人\n",
      "亦無法準用民事訴訟法第137條規定將文書付與有辨別事理能力之同居人或受僱人者\n",
      "得將文書寄存送達地之自治或警察機關\n",
      "並作送達通知書2份\n",
      "1份黏貼於應受送達人住居所、事務所、營業所或其就業處所門首\n",
      "另一份置於該送達處所信箱或其他適當位置\n",
      "以為送達\n",
      "又寄存送達\n",
      "自寄存之日起\n",
      "經10日發生效力\n",
      "刑事訴訟法第62條準用民事訴訟法第136條至138條規定甚明；而民事訴訟法第138條第2項所稱之\n",
      "係指\n",
      "二、經查：本件上訴人即被告蔡銀英違反銀行法案件\n",
      "其於本院審理時陳明其住所為金門縣○○鎮○○路00號\n",
      "有本院歷次準備程序筆錄、審判筆錄附卷可稽\n",
      "而本院於民國104年12月31日以104年度金訴字第1號刑事判決判處有期徒刑1年8月\n",
      "緩刑3年\n",
      "並應於判決確定1年內\n",
      "向公庫支付新臺幣8萬元\n",
      "犯罪所得新臺幣<unk>元沒收\n",
      "該判決於105年1月6日寄存送達被告住所所在之金門縣警察局金湖分局金湖派出所\n",
      "被告本人於105年1月7日15時14分許前往該警局領取等情\n",
      "有個人戶籍資料查詢結果、本院送達證書、入出境資訊連結作業、金門縣警察局金湖分局金湖派出所受理司法文書寄存登記簿各1份在卷可查\n",
      "又被告於斯時並未因案在監執行或羈押\n",
      "此有臺灣高等法院在監在押全國紀錄表1份附卷可據\n",
      "是被告並非在監獄或看守所之人\n",
      "其主張該判決應囑託監所長官為送達云云\n",
      "顯非可採\n",
      "則依上開送達情形\n",
      "該判決正本既已經被告於105年1月7日前往金湖派出所具領\n",
      "是本件上訴期間應自105年1月8日起算10日\n",
      "再依法院訴訟當事人在途期間標準第2條之規定\n",
      "加計在途期間1日\n",
      "是其上訴期間之末日應為105年1月18日屆滿\n",
      "詎上訴人遲至107年3月13日始向本院提起上訴\n",
      "有刑事上訴狀上本院收狀戳章為憑\n",
      "顯已逾越法定上訴期間\n",
      "且無從補正\n",
      "揆諸首揭法律規定\n",
      "應予駁回\n",
      "三、應依刑事訴訟法第362條前段\n",
      "裁定如主文\n",
      "中華民國107年3月28日刑事第二庭\n",
      "\n",
      "Predict lawNames : \n",
      "刑事訴訟法362\n",
      "刑事訴訟法349\n",
      "\n",
      "True lawNames : \n",
      "刑事訴訟法62\n",
      "刑事訴訟法362\n",
      "民事訴訟法137\n",
      "民事訴訟法138 2\n",
      "刑事訴訟法349\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "test_subsampler = torch.utils.data.SubsetRandomSampler(np.array([idx]))\n",
    "test_loader = DataLoader(datasets, batch_size=1,\n",
    "                                  collate_fn=datasets.collate_fn,\n",
    "                                  sampler=test_subsampler)\n",
    "\n",
    "for test_data in test_loader:\n",
    "    documents = test_data[0].to(device)\n",
    "    print(\"Judgement : \")\n",
    "    for sent in documents[0]:\n",
    "        for word in sent:\n",
    "            if word == 0:\n",
    "                continue\n",
    "            print(idx2word[word.cpu().item()],end = '')\n",
    "        print()\n",
    "    sentences_per_document = test_data[1].to(device)\n",
    "    words_per_sentence = test_data[2].to(device)\n",
    "    scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n",
    "                                                             words_per_sentence)\n",
    "    print('\\nPredict lawNames : ')\n",
    "    for idx,s in enumerate(scores[0]):\n",
    "        if s>0.5:\n",
    "            print(idx2tag[idx])\n",
    "#             p_law.append(idx)\n",
    "    print('\\nTrue lawNames : ')\n",
    "    for idx,s in enumerate(test_data[3][0]):\n",
    "        if s != 0:\n",
    "            print(idx2tag[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-marsh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
